{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to find names of companies having profit warnings and save it to data.csv\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "data_name = []\n",
    "data_date = []\n",
    "data_symbol = []\n",
    "data_curr = []\n",
    "data_range = []\n",
    "data_period=[]\n",
    "\n",
    "def scrapperData():\n",
    "    \n",
    "    a=soup.find_all('div', attrs={\"data-th\":\"Co. Name\"})\n",
    "\n",
    "    for data in a:\n",
    "        data_name.append(data.a.string)\n",
    "\n",
    "    a=soup.find_all('div', class_='tdDate')\n",
    "    \n",
    "    for data in a:\n",
    "        data_date.append(data.string)\n",
    "        \n",
    "    a=soup.find_all('div', attrs={\"data-th\":\"Symbol\"})\n",
    "    \n",
    "    for data in a:\n",
    "        data_symbol.append(data.a.string)\n",
    "        \n",
    "    a=soup.find_all('div', attrs={\"data-th\":\"Curr. Est\"})\n",
    "\n",
    "    for data in a:\n",
    "        data_curr.append(data.string)\n",
    "    \n",
    "    a=soup.find_all('div', attrs={\"data-th\":\"New Range\"})\n",
    "\n",
    "    for data in a:\n",
    "        data_range.append(data.string)\n",
    "        \n",
    "    a=soup.find_all('div', attrs={\"data-th\":\"Period\"})\n",
    "\n",
    "    for data in a:\n",
    "        data_period.append(data.string)\n",
    "        \n",
    "\n",
    "str1= \"http://www.rttnews.com/Calendar/ProfitWarnings.aspx?PageNum=\"\n",
    "str2=\"1\"\n",
    "\n",
    "link= str1 + str2\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        page = urllib2.urlopen(link)\n",
    "        soup = BeautifulSoup(page)\n",
    "        scrapperData()\n",
    "        str2=int(str2)\n",
    "        str2=str2+1\n",
    "        str2=str(str2)\n",
    "        link=str1+str2\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        print \"Page Not Found!\"\n",
    "        break\n",
    "        \n",
    "df=pd.DataFrame(data_date,columns=['Date'])\n",
    "df['Symbol']=data_symbol\n",
    "df['Co.name']=data_name\n",
    "df['Curr.Est']=data_curr\n",
    "df['New Range']=data_range\n",
    "df['Period']=data_period\n",
    "\n",
    "file_name='data.csv'\n",
    "df.to_csv(file_name, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to login to the RTTNEWS site\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pickle\n",
    "import numpy as np\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # new window will not open, data scarrping will be done headless\n",
    "chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options, executable_path=\"/path/to/chromedriver\")\n",
    "\n",
    "# Now just tell it wherever you want it to go\n",
    "driver.get(\"https://seekingalpha.com/account/login\")\n",
    "\n",
    "# enter details if you want to have full access to the transcipts otherwise publicly available data will be scraped\n",
    "username = driver.find_element_by_id(\"login_user_email\")\n",
    "password = driver.find_element_by_id(\"login_user_password\")\n",
    "\n",
    "username.send_keys(\"userName\")\n",
    "password.send_keys(\"password\")\n",
    "\n",
    "# driver.find_element_by_value(\"Sign in\").click()\n",
    "# submit_button.click()\n",
    "password.submit()\n",
    "driver.implicitly_wait(5)\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to scarpe detailed transcript of each company name having profit warning whose name is saved in data.csv\n",
    "import numpy as np\n",
    "b=np.loadtxt(r'data.csv',dtype=list,delimiter='\\t',skiprows=1,usecols=(0,))\n",
    "type(b)\n",
    "\n",
    "a=[]\n",
    "for i in b:\n",
    "    if (i not in a):\n",
    "    a.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape full page\n",
    "for sym in a:\n",
    "    search = driver.find_element_by_id('hd-auto')\n",
    "    search.send_keys(sym)\n",
    "    search.send_keys(Keys.RETURN)\n",
    "\n",
    "    time.sleep(2)\n",
    "    elm = driver.find_element_by_link_text(\"Earnings\")\n",
    "    elm.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "    a=driver.find_element_by_link_text('Transcripts')\n",
    "    a.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "    links=[]\n",
    "    elems = driver.find_elements_by_xpath(\"//a[@href]\")\n",
    "    for elem in elems:\n",
    "    links.append(elem.get_attribute(\"href\"))\n",
    "\n",
    "    # print links\n",
    "\n",
    "    time.sleep(2)\n",
    "    search_links=[]\n",
    "    str1=\"results-earnings-call-transcript\"\n",
    "\n",
    "    for i in links:\n",
    "    if(str1 in i):\n",
    "    search_links.append(i)\n",
    "\n",
    "    for j in range(0,min(8,len(search_links))):\n",
    "    driver.get(search_links[j])\n",
    "\n",
    "    temp=[]\n",
    "    temp=search_links[j]\n",
    "\n",
    "    temp = temp.encode('utf-8')\n",
    "    temp=temp[-40:-33] \n",
    "\n",
    "    driver.find_element_by_link_text('Single page view').click()\n",
    "\n",
    "    time.sleep(2)\n",
    "    ll=[]\n",
    "\n",
    "    data2 = \"\"\n",
    "\n",
    "    for i in range(25):\n",
    "        elems = driver.find_elements_by_xpath(\"//p[@class = 'p p%d']\" % i)\n",
    "\n",
    "\n",
    "    for elem in elems:\n",
    "        data2 += elem.text\n",
    "        data2 = data2.encode('utf-8')\n",
    "\n",
    "    fn=sym+\"-\"+temp+\".txt\"\n",
    "    with open(\"/path/to/Data_transcript/\" + fn, \"w+\") as fp: \n",
    "\n",
    "    fp.write(data2)\n",
    "    fp.write(\"\\n\")\n",
    "    fp.write(\"\\n\")\n",
    "    fp.write(\"\\n\")\n",
    "\n",
    "    print \"%s done: %d\" %(sym,j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
